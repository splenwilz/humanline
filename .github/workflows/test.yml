# CI/CD Pipeline for Humanline Backend Testing
# Following GitHub Actions and FastAPI best practices for 2024
# 
# This workflow implements:
# - Matrix testing across Python versions
# - Comprehensive test coverage reporting
# - Security scanning with CodeQL
# - Performance testing
# - Dependency vulnerability scanning
# - Code quality checks

name: ğŸ§ª Test Suite

on:
  push:
    # Run on all branches
    paths:
      - 'backend/**'
      - '.github/workflows/test.yml'
  pull_request:
    # Run on PRs to any branch
    paths:
      - 'backend/**'
      - '.github/workflows/test.yml'
  workflow_dispatch: # Allow manual triggering

# Ensure only one workflow runs at a time for the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Job 1: Code Quality and Security Checks
  code-quality:
    name: ğŸ” Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Full history for better analysis
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv sync --dev
    
    - name: ğŸ” Lint with Ruff (Fast Python Linter)
      working-directory: ./backend
      run: |
        uv run ruff check . --output-format=github
        uv run ruff format --check .
      continue-on-error: true # Don't fail the build, just report
    
    - name: ğŸ”’ Security Scan with Bandit
      working-directory: ./backend
      run: |
        uv add --dev bandit[toml]
        uv run bandit -r . -f json -o bandit-report.json || true
        uv run bandit -r . -f txt || true
      continue-on-error: true
    
    - name: ğŸ“Š Upload Security Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-report
        path: backend/bandit-report.json

  # Job 2: Matrix Testing Across Python Versions
  test-matrix:
    name: ğŸ§ª Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15
    needs: code-quality
    
    env:
      # Set database URLs at job level for all steps
      # Use async drivers for all databases (required by FastAPI + SQLAlchemy async)
      DATABASE_URL: ${{ matrix.os == 'ubuntu-latest' && 'postgresql+asyncpg://testuser:testpassword@localhost:5432/testdb' || 'sqlite+aiosqlite:///./test.db' }}
      TEST_DATABASE_URL: ${{ matrix.os == 'ubuntu-latest' && 'postgresql+asyncpg://testuser:testpassword@localhost:5432/testdb' || 'sqlite+aiosqlite:///./test.db' }}
      # Required environment variables for Settings class
      SECRET_KEY: "test-secret-key-for-ci-only-not-secure"
      ENVIRONMENT: "testing"
    
    strategy:
      fail-fast: false # Continue testing other versions if one fails
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.12', '3.13']  # Focus on stable recent versions
        exclude:
          # Reduce CI time by testing fewer combinations
          - os: windows-latest
            python-version: '3.13'  # Keep 3.12 on Windows
          - os: macos-latest
            python-version: '3.13'  # Keep 3.12 on macOS
    
    services:
      # PostgreSQL service for integration tests (Linux only)
      # Windows and macOS will use SQLite for testing
      postgres:
        image: ${{ matrix.os == 'ubuntu-latest' && 'postgres:15' || '' }}
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv sync --dev
    
    - name: ğŸ—„ï¸ Set up Test Database
      working-directory: ./backend
      run: |
        # Run database migrations for integration tests
        uv run alembic upgrade head
    
    - name: ğŸ§ª Run Unit Tests
      working-directory: ./backend
      run: |
        uv run pytest tests/unit/ \
          --verbose \
          --tb=short \
          --cov=. \
          --cov-report=xml \
          --cov-report=term-missing \
          --cov-fail-under=90 \
          --junit-xml=junit-unit.xml
    
    - name: ğŸ”— Run Integration Tests
      working-directory: ./backend
      run: |
        uv run pytest tests/integration/ \
          --verbose \
          --tb=short \
          --junit-xml=junit-integration.xml
    
    - name: ğŸ“Š Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.13' && matrix.os == 'ubuntu-latest'
      with:
        file: ./backend/coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false # Don't fail if codecov is down
    
    - name: ğŸ“‹ Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.os }}
        path: |
          backend/junit-*.xml
          backend/coverage.xml
          backend/htmlcov/

  # Job 3: Performance Testing
  performance-test:
    name: âš¡ Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [code-quality]
    
    services:
      # PostgreSQL service for performance tests (Linux only)
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv sync --dev
        uv add --dev locust
    
    - name: ğŸ—„ï¸ Set up Database
      working-directory: ./backend
      env:
        DATABASE_URL: postgresql://testuser:testpassword@localhost:5432/testdb
      run: |
        uv run alembic upgrade head
    
    - name: ğŸš€ Start FastAPI Server
      working-directory: ./backend
      env:
        DATABASE_URL: postgresql://testuser:testpassword@localhost:5432/testdb
      run: |
        uv run uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10 # Wait for server to start
    
    - name: âš¡ Run Performance Tests
      working-directory: ./backend
      run: |
        # Create a simple load test
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        
        class OnboardingUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                # Register and login
                response = self.client.post("/api/v1/auth/register", json={
                    "email": f"test{self.environment.runner.user_count}@example.com",
                    "password": "testpassword123",
                    "first_name": "Test",
                    "last_name": "User"
                })
                if response.status_code == 201:
                    self.token = response.json()["access_token"]
                    self.headers = {"Authorization": f"Bearer {self.token}"}
            
            @task(3)
            def check_onboarding_status(self):
                if hasattr(self, 'headers'):
                    self.client.get("/api/v1/onboarding/status", headers=self.headers)
            
            @task(1)
            def check_domain_availability(self):
                if hasattr(self, 'headers'):
                    self.client.get("/api/v1/onboarding/check-domain/testdomain", headers=self.headers)
        EOF
        
        # Run load test
        uv run locust --host=http://localhost:8000 --users 10 --spawn-rate 2 --run-time 30s --headless --html=performance-report.html
    
    - name: ğŸ“Š Upload Performance Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-report
        path: backend/performance-report.html

  # Job 4: Dependency Security Scan
  dependency-scan:
    name: ğŸ”’ Dependency Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
    
    - name: ğŸ“¦ Install Dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install uv safety
        uv sync --dev
    
    - name: ğŸ” Scan Dependencies with Safety
      working-directory: ./backend
      run: |
        # Export requirements for safety scan
        uv export --format requirements-txt --no-hashes > requirements-export.txt
        safety check --file requirements-export.txt --json --output safety-report.json || true
        safety check --file requirements-export.txt || true
      continue-on-error: true
    
    - name: ğŸ“Š Upload Dependency Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: dependency-scan-report
        path: backend/safety-report.json

  # Job 5: Test Summary and Notifications
  test-summary:
    name: ğŸ“‹ Test Summary
    runs-on: ubuntu-latest
    needs: [code-quality, test-matrix, performance-test, dependency-scan]
    if: always()
    
    steps:
    - name: ğŸ“¥ Download All Artifacts
      uses: actions/download-artifact@v4
    
    - name: ğŸ“Š Generate Test Summary
      run: |
        echo "# ğŸ§ª Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ“‹ Job Results" >> $GITHUB_STEP_SUMMARY
        echo "- Code Quality: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Matrix Tests: ${{ needs.test-matrix.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Tests: ${{ needs.performance-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Dependency Scan: ${{ needs.dependency-scan.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ¯ Coverage & Quality Metrics" >> $GITHUB_STEP_SUMMARY
        echo "- Test Coverage: Available in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Security Scan: Available in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Report: Available in artifacts" >> $GITHUB_STEP_SUMMARY
    
    - name: âœ… Mark as Success
      if: needs.code-quality.result == 'success' && needs.test-matrix.result == 'success'
      run: echo "All critical tests passed! âœ…"
    
    - name: âŒ Mark as Failed
      if: needs.code-quality.result == 'failure' || needs.test-matrix.result == 'failure'
      run: |
        echo "Critical tests failed! âŒ"
        exit 1
